{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weili Cao Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_tr = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Over every single \n",
    "def polyline_to_trip_duration(polyline):\n",
    "  return max(polyline.count(\"[\") - 2, 0) * 15\n",
    "\n",
    "df_tr[\"LEN\"] = df_tr[\"POLYLINE\"].apply(polyline_to_trip_duration)\n",
    "df_tr[\"LEN\"] = df_tr[\"POLYLINE\"].apply(polyline_to_trip_duration)\n",
    "\n",
    "from datetime import datetime\n",
    "def parse_time(x):\n",
    "  # We are using python's builtin datetime library\n",
    "  # https://docs.python.org/3/library/datetime.html#datetime.date.fromtimestamp\n",
    "\n",
    "  # Each x is essentially a 1 row, 1 column pandas Series\n",
    "  dt = datetime.fromtimestamp(x[\"TIMESTAMP\"])\n",
    "  return dt.year, dt.month, dt.day, dt.hour, dt.weekday()\n",
    "\n",
    "# Because we are assigning multiple values at a time, we need to \"expand\" our computed (year, month, day, hour, weekday) tuples on \n",
    "# the column axis, or axis 1\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html\n",
    "df_tr[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\"]] = df_tr[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")\n",
    "\n",
    "def mean_absolute_error(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)  # Convert y_true to float32\n",
    "    return tf.reduce_mean(tf.abs(y_pred - y_true))\n",
    "\n",
    "class EpochEndCallback(Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        elapsed_time = time.time() - self.start_time\n",
    "        loss = logs.get('loss')\n",
    "        print(f'Epoch {epoch + 1}: loss = {loss:.4f}, time = {elapsed_time:.2f} seconds')\n",
    "        \n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)  # Convert y_true to float32\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_tr.copy()\n",
    "df['ORIGIN_CALL'].fillna(0, inplace=True)\n",
    "df['ORIGIN_STAND'].fillna(0, inplace=True)\n",
    "df = df[df['MISSING_DATA'] != True]\n",
    "df['YR'] = df['YR'].astype(str)\n",
    "df['MON'] = df['MON'].astype(str)\n",
    "df['DAY'] = df['DAY'].astype(str)\n",
    "df['HR'] = df['HR'].astype(str)\n",
    "df['WK'] = df['WK'].astype(str)\n",
    "df['ORIGIN_CALL'] = df['ORIGIN_CALL'].astype(str)\n",
    "df['ORIGIN_STAND'] = df['ORIGIN_STAND'].astype(str)\n",
    "string_features = ['CALL_TYPE', 'ORIGIN_STAND', 'DAY_TYPE', 'YR', 'MON', 'DAY', 'HR', 'WK'] \n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('string', OneHotEncoder(sparse_output=False), string_features)\n",
    "    ])\n",
    "X = df.drop(columns=['TRIP_ID','ORIGIN_CALL', 'TAXI_ID', 'TIMESTAMP', 'MISSING_DATA','POLYLINE','LEN'])\n",
    "y = df['LEN']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_shape=(X_train_preprocessed.shape[1],)))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), \n",
    "              loss=root_mean_squared_error, metrics=[root_mean_squared_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epoch_end_callback = EpochEndCallback()\n",
    "model.fit(X_train_preprocessed, y_train, epochs=5, batch_size=32, verbose=0, callbacks=[epoch_end_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test_preprocessed)\n",
    "test_loss, test_rmse = model.evaluate(X_test_preprocessed, y_test)\n",
    "print(f\"Test loss: {test_loss}, Test RMSE: {test_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"test_public.csv\")\n",
    "df_test[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\"]] = df_test[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")\n",
    "df_test['ORIGIN_CALL'].fillna(0, inplace=True)\n",
    "df_test['ORIGIN_STAND'].fillna(0, inplace=True)\n",
    "df_test = df_test[df_test['MISSING_DATA'] != True]\n",
    "\n",
    "df_test['ORIGIN_CALL'] = df_test['ORIGIN_CALL'].astype(str)\n",
    "df_test['ORIGIN_STAND'] = df_test['ORIGIN_STAND'].astype(str)\n",
    "string_features = ['CALL_TYPE', 'ORIGIN_STAND', 'DAY_TYPE', 'YR', 'MON', 'DAY', 'HR', 'WK'] \n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('string', OneHotEncoder(sparse_output=False), string_features)\n",
    "    ])\n",
    "X_test = df_test.drop(columns=['TRIP_ID','ORIGIN_CALL', 'TAXI_ID', 'TIMESTAMP', 'MISSING_DATA'])\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "predictions = model.predict(X_test_preprocessed)\n",
    "\n",
    "# Sample submission file that is given on kaggle\n",
    "df_sample = pd.read_csv(\"sampleSubmission.csv\")\n",
    "\n",
    "df_sample[\"TRAVEL_TIME\"] = predictions\n",
    "\n",
    "# mean(716.43) -> 792.73593\n",
    "# median(600) -> 784.74219\n",
    "df_sample.to_csv(\"my_pred.csv\", index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My_TF_Sklearn_Kernel",
   "language": "python",
   "name": "tf_sklearn_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
